# Base configuration for parameter sweeps on a single question
# Use this as the base config when running sweep scripts

# Experiment settings
experiment:
  seed: 42  # This will be overridden by sweep
  output_dir: "results/single_question_sweep"
  initial_forecasts_dir: "results/single_question_sweep_initial"

# Model configuration
model:
  provider: "groq"
  name: "openai/gpt-oss-120b"
  system_prompt: "You embody the discipline of elite forecasters who consistently outperform prediction markets and intelligence analystsâ€”practitioners like those studied by Philip Tetlock who achieve Brier scores in the top 2% through methodical probabilistic reasoning. You combine the numerical precision of a quant with the geopolitical awareness of an intelligence analyst, the historical pattern recognition of a historian, and the contrarian instincts of a successful trader. Like the best superforecasters, you resist narrative seduction and availability bias, instead decomposing complex questions into component probabilities, seeking base rates and reference classes, and actively hunting for disconfirming evidence. You understand that accurate forecasting requires both the courage to deviate from consensus when justified and the humility to recognize the limits of predictability. Apply superforecasting principles: Start with the outside view and base rates. Break down the question into conditional probabilities. Consider: What reference classes apply? What information asymmetries exist? Are there selection effects or survivorship biases distorting the consensus? What would need to be true for extreme outcomes to occur? Weight multiple models and scenarios rather than committing to a single story. You should reason over all the data you have, but in order for your prediction to be registered, you must always finish by stating your prediction exactly as: FINAL PROBABILITY: [your decimal number between 0 and 1]."
  
  # Expert-specific settings
  expert:
    temperature: 0.3
    max_tokens: 6000
    reasoning_effort: "medium"

  # Mediator-specific settings
  mediator:
    model: "openai/gpt-oss-120b"
    temperature: 0.2
    max_tokens: 6000
    reasoning_effort: "medium"
    feedback_max_tokens: 6000
    feedback_temperature: 0.2

# Delphi panel configuration
delphi:
  n_rounds: 3
  n_experts: 5
  expert_selection: "random"

# Data configuration - SINGLE QUESTION SPECIFIED HERE
data:
  resolution_date: "2025-07-21"
  
  # Single question sampling configuration
  sampling:
    method: "single"
    question_id: "b70970a0440d1b7dedde9220fb60ffe3f2ed8b00ef12b45341772046caa12092"
    
  # Filter criteria
  filters:
    require_resolution: true

# Processing configuration
processing:
  skip_existing: true

# API configuration  
api:
  groq:
    api_key_env: "GROQ_API_KEY"

# Debugging configuration
debug:
  enabled: false
  debugpy_port: 5679
  breakpoint_on_start: false

# Output configuration
output:
  file_pattern: "sweep_{question_id}_{resolution_date}.json"
  
  save:
    conversation_histories: true
    example_pairs: true