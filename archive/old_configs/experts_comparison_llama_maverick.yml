# Expert Comparison: meta-llama/llama-4-maverick-17b-128e-instruct (Groq)
# This configuration is part of the expert model comparison study

# Experiment settings
experiment:
  seed: 42  # Random seed for reproducibility
  output_dir: "results/experts_comparison_llama_maverick"
  initial_forecasts_dir: "results/experts_comparison_llama_maverick_initial"

# Model configuration
model:
  provider: "groq"
  name: "meta-llama/llama-4-maverick-17b-128e-instruct"
  system_prompt: "# Forecasting Configuration\n\nYou operate as an elite forecaster implementing methods proven by top-2% practitioners (per Tetlock's research). Core approach: combine quantitative precision with multi-domain analysis while actively resisting narrative bias.\n\n## Methodology\n\nApply systematic superforecasting principles:\n- Start with base rates and reference classes\n- Decompose questions into conditional probabilities  \n- Identify information asymmetries and selection effects\n- Weight multiple models rather than committing to single narratives\n- Explicitly consider extreme-outcome requirements\n\nEvaluate all available data through this framework, but maintain epistemic humility about predictability limits. \n\n## Output Format\n\nAfter presenting your analysis, always conclude with:\n\n```\nFINAL PROBABILITY: [decimal between 0 and 1]\n```"
  expert:
    temperature: 0.3
    max_tokens: 6000
    
  # Mediator-specific settings (overrides base model settings)
  mediator:
    model: "meta-llama/llama-4-maverick-17b-128e-instruct"
    temperature: 0.2
    max_tokens: 6000
    feedback_max_tokens: 6000
    feedback_temperature: 0.2

# Delphi panel configuration
delphi:
  n_rounds: 3  # Number of Delphi rounds
  n_experts: 5  # Maximum number of experts per panel
  expert_selection: "random"  # How to select experts when more than n_experts available

# Data configuration
data:
  resolution_date: "2025-07-21"  # Date for which to get resolutions
  
  # Question sampling
  sampling:
    method: "random"  # Options: by_topic, random, first
    n_per_topic: 2  # Number of questions per topic (for by_topic method)
    n_questions: 3 # Total number of questions to sample (for random/first methods)
    
  # Filter criteria
  filters:
    require_resolution: true  # Only include questions with resolutions on resolution_date

# Processing configuration
processing:
  skip_existing: true  # Skip if output file already exists

# API configuration  
api:
  groq:
    api_key_env: "GROQ_API_KEY"  # Environment variable name for Groq API key

# Debugging configuration
debug:
  enabled: false  # Enable debug mode (for debugpy attachment)
  debugpy_port: 5679  # Port for debugpy attachment
  breakpoint_on_start: false  # Add breakpoint at start of execution

# Output configuration
output:
  # File naming pattern - available variables: {question_id}, {resolution_date}
  file_pattern: "experts_comparison_llama_maverick_{question_id}_{resolution_date}.json"
  
  # What to save in output files
  save:
    conversation_histories: true  # Include full conversation histories
    example_pairs: true  # Include the example pairs used for each expert