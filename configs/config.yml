# Delphi Evaluation Configuration

experiment:
  name: "delphi_evaluation"
  timestamp_format: "%Y%m%d_%H%M%S"
  results_dir: "results/run_{timestamp}"

model:
  provider: "claude"  # Options: openai, claude, anthropic
  name: "claude-4-haiku"  # Options: gpt-4o, gpt-4o-mini, claude-4-haiku, claude-4-sonnet, claude-4-opus
  system_prompt: "You are an expert superforecaster skilled at making calibrated probability estimates. You carefully analyze available information, consider base rates, and avoid overconfidence. You think in concrete probabilities rather than vague terms."
  max_tokens: 2000
  temperature: 0.3

evaluation:
  n_questions: 50
  n_experts_range: [1, 2, 4, 8, 16]
  max_workers: 16  # Number of parallel API calls
  random_seed: 42  # Seed for reproducible question sampling
  
panel:
  condition_on_data: true  # Whether to condition on human forecaster data
  delphi_rounds: 1  # Number of Delphi rounds (1 or 2). If 2, experts see each other's round 1 responses
  
plotting:
  dpi: 150
  figure_size: [10, 6]
  aggregate_figure_size: [14, 6]
  
output:
  save_detailed_results: true
  save_csv_summary: true
  create_plots: true
  create_aggregate_plots: true 